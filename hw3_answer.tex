\documentclass{article}
\usepackage{import}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{mathtools}

\subimport*{./}{macro}
\setlength\parindent{0px}

\begin{document}
\title{Homework \#3}
\author{
    \normalsize{CSE 446/546: Machine Learning}\\
    \normalsize{Zuo Wang}\\
}
\date{{}}
\maketitle


% Quesetion 1
\begin{spacing}{3}{\bfseries\LARGE Question 1}\end{spacing}
\textbf{(a.)}
\begin{flushleft}
    We should decrease $\sigma$ because decreasing the bandwith allows us to have a more localized and complex model.
    Mathmatically speaking, decreasing $\sigma$ makes the kernel value smaller (exponential of a larger negative value)
    therefore the penalty term is smaller, resulting in a more complex model.
\end{flushleft}

\vspace{5mm}\textbf{(b.)}
\begin{flushleft}
    False. Most neural networks actually have non-convex loss functions.
\end{flushleft}

\vspace{5mm}\textbf{(c.)}
\begin{flushleft}
    False. If we initialize all weights to zero when training a deep neural network, the gradient of all the weights will
    be the same during backpropagation, resulting in all neurons learning the same features and updating their weights identically.
\end{flushleft}

\vspace{5mm}\textbf{(d.)}
\begin{flushleft}
    True. Linear activation functions would result in a network that can only learn linear decision boundaries.
\end{flushleft}

\vspace{5mm}\textbf{(e.)}
\begin{flushleft}
    True. The backward pass step requires computing the gradients of the loss with respect to the parameters, 
    which involves matrix multiplications, and derivating activation function. And that is more time-consuming compared to the forward pass step.
\end{flushleft}

\vspace{5mm}\textbf{(f.)}
\begin{flushleft}
    False. While neural network is consider one of the best models in most cases, it is not always the best choice for every circumstance.
    Generally speaking, neural networks are more complex, require a longer runtime, are better for larger dataset, and tend to overfit. 
    In some cases, simple models with regularization might just perform better.
\end{flushleft}


% Quesetion 2
\pagebreak
\begin{spacing}{3}{\bfseries\LARGE Question 2}\end{spacing}
\textbf{(a.)}
\begin{center}
    $\includegraphics {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a2a.png}$
\end{center}
As shown, the support vectors are (0, 0.5), (2, 1), and (1, 2)

\pagebreak
\vspace{5mm}\textbf{(b.)}
\begin{flushleft}
    From the support vectors and their corresponding labels, we know that:
\end{flushleft}
\begin{center}
    $\begin{bmatrix}1 \\ 2 \\\end{bmatrix} w + b = -1$,
    $\begin{bmatrix}0 \\ 0.5 \\\end{bmatrix} w + b = 1$,
    $\begin{bmatrix}2 \\ 1 \\\end{bmatrix} w + b = 1$
\end{center}
Looking at the last two equations, we can construct the matrix:
\begin{align*}
    w &= \left[\begin{array}{@{}cc|c@{}}
        0 & 0.5 & 1-b \\
        2 & 1 & 1-b
    \end{array}\right]
    \\&= \left[\begin{array}{@{}cc|c@{}}
        2 & 1 & (1-b) \\
        0 & 1 & 2(1-b)
    \end{array}\right]
    \\&= \left[\begin{array}{@{}cc|c@{}}
        1 & 0 & -\frac{1}{2}(1-b) \\
        0 & 1 & 2(1-b)
    \end{array}\right]
    \\&= \left[-\frac{1}{2}(1-b) \;\;\;\; 2(1-b)\right]
\end{align*}
And we plug $w$ back into $\begin{bmatrix}1 \\ 2 \\\end{bmatrix} w + b = -1$:
\begin{align*}
    \begin{bmatrix}1 \\ 2 \\\end{bmatrix} \cdot \left[-\frac{1}{2}(1-b) \;\;\;\; 2(1-b)\right] + b &= -1
    \\ -\frac{1}{2}(1-b) + 4(1-b) + b &= -1
    \\ \frac{7}{2}(1-b) + b &= -1
    \\ -\frac{5}{2} b &= -\frac{9}{2}
    \\ b &= \frac{9}{5}
\end{align*}
\begin{center}
    \boxed{
    b = \frac{9}{5} \;\;
    w = \left[\frac{2}{5} \;\;\;\; -\frac{8}{5}\right]}
\end{center}

\pagebreak
\vspace{5mm}\textbf{(c.)}
\begin{flushleft}
    Denote the two hyperplanes $x^T w + b = 1$ and $x^T w + b = -1$ as $H_1$ and $H_2$, respectively.\\
    Let's pick a random point $x_0$ on $H_1$, we know that 
    Now, the projection of $x_0$ onto $H_1$ is given by $P_H(x_0) = x_0 - \frac{w^Tx_0 - c}{||w||^2_2}w$ 
    where the offset $c = w^Tx_0 = -1 - b$.\\
    And since the distance between $H_1$ and $H_2$ is euqal to the distance $x_0$ on $H_1$ and it's projection on $H_2$,
    we can just calculate $||x_0 - P_H(x_0)||_2$ to determine the distance between $H_1$ and $H_2$:
\end{flushleft}
\begin{align*}
    \norm2{x_0 - P_H(x_0)}^2 &= \norm2{x_0 - \left(x_0 - \frac{w^Tx_0 - c}{||w||^2_2}w\right)}\\
    &=\norm2{\frac{w^Tx_0 - (-1 - b)}{||w||^2_2}w} \\
    &=\norm2{\frac{(1 - b) + 1 + b}{||w||^2_2}w} \\
    &=\norm2{\frac{2}{w}} \\
    &=\frac{2}{\norm2{w}} \\
\end{align*}
\begin{flushleft}
    Therefore, the distance between the two separating hyperplanes is $\frac{2}{\norm2{w}}$
\end{flushleft}


% Quesetion 3
\pagebreak
\begin{spacing}{3}{\bfseries\LARGE Question 3}\end{spacing}
\begin{align*}
    \phi (x) \cdot \phi (x') &= \sum_{i=0}^{\infty}
    \left( \frac{1}{\sqrt{i!}} e^{\frac{-x^2}{2}}x^i \right) \cdot 
    \left( \frac{1}{\sqrt{i!}} e^{\frac{-x'^2}{2}}(x')^i \right)
    \\&= \sum_{i=0}^{\infty} \frac{1}{i!} e^{\frac{-(x^2+x'^2)}{2}} x^i(x')^i
    \\&= e^{\frac{-(x^2+x'^2)}{2}}
    \sum_{i=0}^{\infty} \frac{1}{i!} x^i(x')^i
\end{align*}
Because the Taylor expansion of $e^z = \sum_{n=0}^{\infty} \frac{1}{n!}z^n$, 
the summation above just becomes $e^{xx'}$, therefore:
\begin{align*}
    \phi (x) \cdot \phi (x') &= e^{\frac{-(x^2+x'^2)}{2}} \cdot e^{xx'}
    \\&= e^{\frac{-x^2 -x'^2 + 2xx'}{2}}
    \\&= e^{\frac{-(x - x')^2}{2}}
\end{align*}

% Quesetion 4
\pagebreak
\begin{spacing}{4}{\bfseries\LARGE Question 4}\end{spacing}
\textbf{(a.)}
\begin{flushleft}
    For the RBF kernel, $\lambda_{optimal} = 10^{-3}$, $\;\gamma_{optimal} = 10.5416$\\
    For the polynomial kernel, $\lambda_{optimal} = 10^{-3}$, $\;d_{optimal} = 20.5102$
\end{flushleft}

\vspace{5mm}\textbf{(b.)}
\begin{center}
    $\includegraphics[width=0.7\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a4_rbf.png}$
    $\includegraphics[width=0.7\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a4_poly.png}$
\end{center}


% Quesetion 5
\pagebreak
\begin{spacing}{3}{\bfseries\LARGE Question 5}\end{spacing}
\textbf{(b.)}
\begin{center}
    $\includegraphics[width=0.8\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a5b_mse.png}$
    $\includegraphics[width=0.8\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a5b_ce.png}$
\end{center}

\vspace{5mm}\textbf{(c.)}
\begin{center}
    $\includegraphics[width=0.8\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a5c_mse.png}$
    \\Best model for MSE loss: Hidden $\rightarrow$ ReLU $\rightarrow$ Hidden $\rightarrow$ Sigmoid (accuracy socre of 0.8764)
    $\includegraphics[width=0.8\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a5c_ce.png}$
    \\Best model for Cross Entropy loss: Hidden $\rightarrow$ Sigmoid $\rightarrow$ Hidden $\rightarrow$ ReLU (accuracy socre of 0.7398)

\end{center}

% Quesetion 6
\pagebreak
\begin{spacing}{3}{\bfseries\LARGE Question 6}\end{spacing}
\textbf{(a.)}
\begin{center}
    $\includegraphics[width=0.7\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a6a.png}$
\end{center}
Loss: 0.09190159004156331\\
Accuracy: 0.9736

\vspace{5mm}\textbf{(b.)}
\begin{center}
    $\includegraphics[width=0.7\textwidth] {/Users/zuo/Desktop/School/SP 23/CSE 446/hw3/figures/a6b.png}$
\end{center}
Loss: 0.15115708102590775\\
Accuracy: 0.9674

\vspace{5mm}\textbf{(c.)}
\begin{flushleft}
    The number of parameters for F1 is 50890, and it's 26506 for F2.\\
    In comparison to F2, F1 model is more complex, achieved higer accuracy and lower loss, and converged in less epoches.
    Therefore in this case, I would say that F1(wide \& shallow) is better. 
    However, we can only conclude that for this specific dataset. 
    Both approaches have their own advantages and considerations, and their performance, again, depend on the specific problem and dataset we're dealing with.
\end{flushleft}

\end{document}